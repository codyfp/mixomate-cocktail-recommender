{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Food.com Dataset Reccomender Binary Matrix\n",
    "Below is data exploration on the foolwoing dataset https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions?select=RAW_recipes.csv \n",
    "\n",
    "This dataset consists of 180K+ recipes and 700K+ recipe reviews covering 18 years of user interactions and uploads on Food.com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import zipfile\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "#from surprise import SVD\n",
    "#from surprise import Dataset\n",
    "#from surprise import Reader\n",
    "#from surprise.model_selection import train_test_split\n",
    "#from surprise import accuracy\n",
    "#import tensorflow.keras as tf\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "From the zipped dataset we extract the recipes and the interactions (reviews).\n",
    "Below is how we got the data from the original very large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip_file_path = \"archive.zip\"\n",
    "# csv_file_name1 = 'RAW_recipes.csv'\n",
    "# csv_file_name2 = 'RAW_interactions.csv' \n",
    "\n",
    "# # Initialize two DataFrames to store the data from the two CSV files\n",
    "# df1 = None\n",
    "# df2 = None\n",
    "\n",
    "# # Open the zip file and read the first CSV file into the first DataFrame\n",
    "# with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "#     with zip_file.open(csv_file_name1) as csv_file_in_zip:\n",
    "#         df_recipes = pd.read_csv(csv_file_in_zip)\n",
    "\n",
    "# # Open the zip file again and read the second CSV file into the second DataFrame\n",
    "# with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "#     with zip_file.open(csv_file_name2) as csv_file_in_zip:\n",
    "#         df_reviews = pd.read_csv(csv_file_in_zip)\n",
    "\n",
    "# #We renamed the id column to match to id in reviews so we can merge on this later\n",
    "# df_recipes.rename(columns={'id': 'recipe_id'}, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #We combine both the dataframes\n",
    "\n",
    "# combined_df = pd.merge(df_reviews, df_recipes, on='recipe_id', how='inner')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection\n",
    "\n",
    "Below we search the tags that indicate the recipe is for a cocktail. This cuts downs are dataset from 180K recipes to around 4K, significiantly reducing our file size. We have used this exported csv as our base dataset so we don't have to rely on git lfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df = combined_df[combined_df['tags'].str.contains('cocktails', case=False, na=False)]\n",
    "\n",
    "# combined_df.rename(columns={'id': 'recipe_id'}, inplace=True)\n",
    "\n",
    "# #We drop columns that we don't need\n",
    "# columns_to_drop = ['date', 'review','minutes','nutrition','contributor_id', 'description','submitted']\n",
    "\n",
    "# combined_df = combined_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# combined_df.to_csv(\"cocktail_dataset.csv\")\n",
    "# #Exporting a refined dataset to be used in mongoDB\n",
    "\n",
    "# to_keep = ['recipe_id','rating','name','n_steps','ingredients','steps','n_ingredients']\n",
    "\n",
    "# mongo_csv = combined_df[to_keep].drop_duplicates(subset=['name'])\n",
    "\n",
    "\n",
    "# mongo_csv.to_csv(\"cocktail_data_mongo.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "We are exploring the cocktail counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df = pd.read_csv(\"cocktail_dataset.csv\")\n",
    "\n",
    "# cocktail_review_counts = combined_df.groupby('recipe_id')['rating'].count().reset_index()\n",
    "# cocktail_review_counts.columns = ['recipe_id', 'review_count']\n",
    "# # Sort the DataFrame by review_count in descending order\n",
    "# # Sort the review counts DataFrame by review_count in descending order\n",
    "# cocktail_review_counts_sorted = cocktail_review_counts.sort_values(by='review_count', ascending=False)\n",
    "\n",
    "# # Merge the review counts DataFrame with the recipes DataFrame to get cocktail names\n",
    "# table_data = cocktail_review_counts_sorted.merge(df_recipes[['recipe_id', 'name']], on='recipe_id', how='inner')\n",
    "\n",
    "# # Display the table\n",
    "# table = tabulate(table_data, headers=['Recipe ID','Review Count','Cocktail Name'], tablefmt='pretty', showindex=False)\n",
    "# print(table)\n",
    "\n",
    "# print(cocktail_review_counts_sorted['review_count'].mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation for Matrix\n",
    "\n",
    "We Strip the regex characters from the ingredients column.\n",
    "\n",
    "We also need to remove the duplicates since one cocktail can have many reviews, and since we combined the reviews and coctail dataframes, there will be duplicates.\n",
    "\n",
    "The duplicates were useful for data exploration, but now exploration is done we need only unique values of the cocktails for our prediction matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only process if the first item in 'ingredients' column is a string\n",
    "if isinstance(combined_df['ingredients'].iloc[0], str):\n",
    "    # Remove [ and ] \n",
    "    combined_df['ingredients'] = combined_df['ingredients'].str.replace('[', '').str.replace(']', '')\n",
    "    # Splitting the ingredients string by commas\n",
    "    combined_df['ingredients'] = combined_df['ingredients'].str.split(',')\n",
    "\n",
    "\n",
    "# Drop duplicates based on the 'name' column and assign the result back to combined_df\n",
    "combined_df = combined_df.drop_duplicates(subset=['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if there are any duplicates before processing for the matrix\n",
    "\n",
    "duplicate_names = combined_df[combined_df['name'].duplicated(keep=False)]\n",
    "print(duplicate_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the 'ingredients' column\n",
    "exploded_df = combined_df.explode('ingredients')\n",
    "\n",
    "# Find unique ingredients\n",
    "unique_ingredients = exploded_df['ingredients'].unique()\n",
    "\n",
    "ingredient_counts = exploded_df['ingredients'].value_counts()\n",
    "\n",
    "exploded_df['ingredients'] = exploded_df['ingredients'].str.replace('','')\n",
    "\n",
    "print(ingredient_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cocktail Similarity\n",
    "## Next We will prepare the features of the data\n",
    "**Feature Extraction**\n",
    "The ingredients list will be the primary feature for our content-based filtering.\n",
    "**One-hot encoding**\n",
    "This converts our categorical data into a numerical format that machine learning algorithms can understand and process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the binarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Filtering out non-iterable items\n",
    "filtered_ingredients = [ingredients if isinstance(ingredients, (list, tuple)) else [] for ingredients in combined_df['ingredients']]\n",
    "\n",
    "# Apply MultiLabelBinarizer on the filtered data\n",
    "binary_matrix = mlb.fit_transform(filtered_ingredients)\n",
    "\n",
    "# Convert the binary matrix into a DataFrame for better visualization and manipulation\n",
    "df_binary = pd.DataFrame(binary_matrix, columns=mlb.classes_)\n",
    "\n",
    "\n",
    "combined_df = pd.concat([combined_df, df_binary], axis=1)\n",
    "\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = cosine_similarity(df_binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_cocktails(input_value, N=5):\n",
    "    \"\"\"\n",
    "    Fetch similar cocktails based on a given cocktail name or ID.\n",
    "    \n",
    "    Args:\n",
    "    - input_value (str or int): Name or ID of the cocktail.\n",
    "    - N (int): Number of similar cocktails to return. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    - list: Names of top N similar cocktails.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine if input is name or ID\n",
    "    if isinstance(input_value, str):\n",
    "        if input_value not in combined_df['name'].values:\n",
    "            raise ValueError(f\"No cocktail named {input_value} found in the dataset.\")\n",
    "        cocktail_index = combined_df[combined_df['name'] == input_value].index[0]\n",
    "    elif isinstance(input_value, int):  # Assuming ID is an integer\n",
    "        if input_value not in combined_df['recipe_id'].values:\n",
    "            raise ValueError(f\"No cocktail with ID {input_value} found in the dataset.\")\n",
    "        cocktail_index = combined_df[combined_df['recipe_id'] == input_value].index[0]\n",
    "    else:\n",
    "        raise ValueError(\"Input value must be either a name (string) or an ID (integer).\")\n",
    "    \n",
    "    # Fetch and enumerate similarity scores for the given cocktail\n",
    "    similar_scores = list(enumerate(similarity_matrix[cocktail_index]))\n",
    "    \n",
    "    # Sort the scores\n",
    "    sorted_similar_scores = sorted(similar_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return the top N cocktail names excluding the input cocktail itself\n",
    "    return [combined_df.iloc[i[0]]['name'] for i in sorted_similar_scores[1:N+1]]\n",
    "\n",
    "get_similar_cocktails(98221)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "\n",
    "The provided code demonstrates an automated process for extracting the first image from Google Image search results based on a list of cocktail-related queries.\n",
    "\n",
    "---\n",
    "\n",
    "### Libraries and Initialization:\n",
    "\n",
    "**Selenium** is used for web automation. It opens a Chrome browser and interacts with web pages.  \n",
    "**Requests** and **io** libraries facilitate the downloading of images from the internet.  \n",
    "**PIL** from the Pillow library aids in image manipulation.\n",
    "\n",
    "---\n",
    "\n",
    "### Main Functions:\n",
    "\n",
    "**1. `get_first_image_from_google`:**  \n",
    "This function searches Google Images with a given query.  \n",
    "It clicks on the first image thumbnail to view it in full.  \n",
    "The direct URL of the image is then extracted and returned.\n",
    "\n",
    "**2. `download_image`:**  \n",
    "Given an image URL, this function fetches the image using the requests library.  \n",
    "The image is then saved to the local disk using the Pillow library.\n",
    "\n",
    "---\n",
    "\n",
    "### Execution:\n",
    "\n",
    "**Queries Creation:**  \n",
    "The code prepares a list of queries named `queries`, where each query is generated by appending \"food.com cocktail\" to each 'name' from the `combined_df` DataFrame.\n",
    "\n",
    "**Image Extraction:**  \n",
    "The code demonstrates two potential methods:  \n",
    "- Looping through the entire list of queries to download images for each one.\n",
    "- Looping through a limited number (e.g., first 5) of queries from the list.\n",
    "\n",
    "In this script, the second method is active. The code performs an image search for the first n cocktails from the list, fetches their first images, and saves them with a numerical filename (0.jpg, 1.jpg, etc.).\n",
    "\n",
    "After all operations, the automated browser session (`wd`) is closed using `wd.quit()`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import io\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "\n",
    "# For now use local path until I add relative path\n",
    "\n",
    "PATH = r\"C:\\Users\\samue\\OneDrive\\Documents\\GitHub\\software-innovation-studio\\modelGeneration\\chromedriver.exe\"\n",
    "wd = webdriver.Chrome(PATH)\n",
    "\n",
    "def get_first_image_from_google(wd, query):\n",
    "    # Format the URL with the given query\n",
    "    url = f\"https://www.google.com/search?q={query}&tbm=isch\"\n",
    "    wd.get(url)\n",
    "    \n",
    "    try:\n",
    "        # Find the first image thumbnail and click it\n",
    "        thumbnail = wd.find_element(By.CLASS_NAME, \"Q4LuWd\")\n",
    "        thumbnail.click()\n",
    "        time.sleep(1)  # Wait for the image to load\n",
    "\n",
    "        # Extract the image URL\n",
    "        image = wd.find_element(By.CLASS_NAME, \"r48jcc\")\n",
    "        if image.get_attribute('src') and 'http' in image.get_attribute('src'):\n",
    "            return image.get_attribute('src')\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def download_image(download_path, url, file_name):\n",
    "    try:\n",
    "        image_content = requests.get(url).content\n",
    "        image_file = io.BytesIO(image_content)\n",
    "        image = Image.open(image_file)\n",
    "        file_path = download_path + file_name\n",
    "\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            image.save(f, \"JPEG\")\n",
    "\n",
    "        print(\"Success\")\n",
    "    except Exception as e:\n",
    "        print('FAILED -', e)\n",
    "\n",
    "queries = [f\"{row['name']} food.com cocktail\" for _, row in combined_df.iterrows()]\n",
    "\n",
    "# This Loops through entire dataset of cocktails\n",
    "# for i in range(len(queries)):\n",
    "#     url = get_first_image_from_google(wd, queries[i])\n",
    "#     if url:\n",
    "#         download_image(\"\", url, f\"{i}.jpg\")\n",
    "\n",
    "#Just does n amount\n",
    "n = 5\n",
    "for i in range(n):\n",
    "    url = get_first_image_from_google(wd, queries[i])\n",
    "    if url:\n",
    "        download_image(\"\", url, f\"{i}.jpg\")\n",
    "\n",
    "\n",
    "wd.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
